{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Comprehensive Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook provides a comprehensive EDA of the retail transaction dataset to understand market behavior patterns.\n",
        "\n",
        "## Objectives\n",
        "- **Summary Statistics**: Descriptive statistics for all key variables\n",
        "- **Distribution Analysis**: Histograms, KDE plots, and boxplots\n",
        "- **Correlation Analysis**: Heatmaps showing relationships between variables\n",
        "- **Missing Data Analysis**: Visual matrix of missing values\n",
        "- **Outlier Detection**: IQR and Z-score methods\n",
        "- **Time-Trend Analysis**: Temporal patterns and market volatility\n",
        "- **Customer Analysis**: RFM-style customer segmentation\n",
        "- **Product Analysis**: Category and product-level insights\n",
        "- **Deep Insights**: Market volatility, seasonal patterns, geographical demand\n",
        "\n",
        "## Required Components\n",
        "1. Summary statistics\n",
        "2. Distribution plots (Histogram, KDE, Boxplot)\n",
        "3. Correlation heatmap\n",
        "4. Missing data matrix\n",
        "5. Outlier detection (IQR, Z-score)\n",
        "6. Time-trend plots\n",
        "7. Customer-level summaries (RFM-style)\n",
        "8. Product/Category-level insights\n",
        "9. Deep insights (volatility, seasonal, geographical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Mount Google Drive (if using Drive to store data)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Alternatively, upload files directly in Colab\n",
        "# # Go to Files -> Upload to upload the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m plt.rcParams[\u001b[33m'\u001b[39m\u001b[33mfigure.figsize\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Set the project root directory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m project_root = \u001b[43mos\u001b[49m.path.abspath(os.path.join(os.getcwd(), \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     17\u001b[39m data_path = os.path.join(project_root, \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mOnline Retail.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load raw data\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Load and prepare dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Set the project root directory\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "data_path = os.path.join(project_root, 'data', 'raw', 'Online Retail.csv')\n",
        "\n",
        "# Load raw data\n",
        "df_raw = pd.read_csv(data_path, encoding='latin-1')\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nRaw Dataset Shape: {df_raw.shape}\")\n",
        "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
        "print(f\"\\nData Types:\\n{df_raw.dtypes}\")\n",
        "\n",
        "# Basic cleaning for EDA (preserve raw for comparison)\n",
        "df = df_raw.copy()\n",
        "\n",
        "# Convert InvoiceDate to datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
        "\n",
        "# Remove canceled orders\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# Remove invalid transactions\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "df = df[df['Description'].notna()]\n",
        "\n",
        "# Create derived features\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
        "df = df[df['InvoiceDate'].notna()]\n",
        "\n",
        "# Temporal features\n",
        "df['Year'] = df['InvoiceDate'].dt.year\n",
        "df['Month'] = df['InvoiceDate'].dt.month\n",
        "df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek\n",
        "df['Hour'] = df['InvoiceDate'].dt.hour\n",
        "df['MonthYear'] = df['InvoiceDate'].dt.to_period('M').astype(str)\n",
        "\n",
        "print(f\"\\nCleaned Dataset Shape: {df.shape}\")\n",
        "print(f\"Rows removed: {len(df_raw) - len(df):,} ({(len(df_raw) - len(df))/len(df_raw)*100:.2f}%)\")\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Summary Statistics for Numeric Variables\n",
        "print(\"=\" * 80)\n",
        "print(\"1. SUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "numeric_cols = ['Quantity', 'UnitPrice', 'TotalPrice']\n",
        "print(\"\\n1.1 Numeric Variables Summary:\")\n",
        "print(df[numeric_cols].describe().T)\n",
        "\n",
        "# Additional statistics\n",
        "print(\"\\n1.2 Additional Statistics:\")\n",
        "stats_df = pd.DataFrame({\n",
        "    'Variable': numeric_cols,\n",
        "    'Mean': [df[col].mean() for col in numeric_cols],\n",
        "    'Median': [df[col].median() for col in numeric_cols],\n",
        "    'Std Dev': [df[col].std() for col in numeric_cols],\n",
        "    'Skewness': [df[col].skew() for col in numeric_cols],\n",
        "    'Kurtosis': [df[col].kurtosis() for col in numeric_cols],\n",
        "    'Min': [df[col].min() for col in numeric_cols],\n",
        "    'Max': [df[col].max() for col in numeric_cols],\n",
        "    'Q1': [df[col].quantile(0.25) for col in numeric_cols],\n",
        "    'Q3': [df[col].quantile(0.75) for col in numeric_cols],\n",
        "    'IQR': [df[col].quantile(0.75) - df[col].quantile(0.25) for col in numeric_cols]\n",
        "})\n",
        "print(stats_df.to_string(index=False))\n",
        "\n",
        "# Categorical summary\n",
        "print(\"\\n1.3 Categorical Variables Summary:\")\n",
        "print(f\"\\nUnique Countries: {df['Country'].nunique()}\")\n",
        "print(f\"Top 10 Countries by Transaction Count:\")\n",
        "print(df['Country'].value_counts().head(10))\n",
        "\n",
        "print(f\"\\nUnique Products: {df['StockCode'].nunique()}\")\n",
        "print(f\"Unique Customers: {df['CustomerID'].nunique():.0f}\")\n",
        "print(f\"Unique Invoices: {df['InvoiceNo'].nunique()}\")\n",
        "\n",
        "print(f\"\\nDate Range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
        "print(f\"Total Revenue: £{df['TotalPrice'].sum():,.2f}\")\n",
        "print(f\"Average Transaction Value: £{df['TotalPrice'].mean():.2f}\")\n",
        "print(f\"Median Transaction Value: £{df['TotalPrice'].median():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Distribution Analysis\n",
        "\n",
        "Histograms, KDE plots, and boxplots to understand the distribution of key variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Distribution Plots: Histogram, KDE, and Boxplot\n",
        "print(\"=\" * 80)\n",
        "print(\"2. DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "fig.suptitle('Distribution Analysis: Histograms, KDE, and Boxplots', fontsize=16, y=1.02)\n",
        "\n",
        "variables = ['Quantity', 'UnitPrice', 'TotalPrice']\n",
        "plot_types = ['Histogram', 'KDE', 'Boxplot']\n",
        "\n",
        "for idx, var in enumerate(variables):\n",
        "    # Histogram\n",
        "    axes[0, idx].hist(df[var], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    axes[0, idx].set_title(f'{var} - Histogram', fontweight='bold')\n",
        "    axes[0, idx].set_xlabel(var)\n",
        "    axes[0, idx].set_ylabel('Frequency')\n",
        "    axes[0, idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # KDE Plot\n",
        "    df[var].plot(kind='kde', ax=axes[1, idx], color='darkblue', linewidth=2)\n",
        "    axes[1, idx].set_title(f'{var} - Kernel Density Estimation', fontweight='bold')\n",
        "    axes[1, idx].set_xlabel(var)\n",
        "    axes[1, idx].set_ylabel('Density')\n",
        "    axes[1, idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Boxplot\n",
        "    bp = axes[2, idx].boxplot(df[var], vert=True, patch_artist=True)\n",
        "    bp['boxes'][0].set_facecolor('lightcoral')\n",
        "    axes[2, idx].set_title(f'{var} - Boxplot', fontweight='bold')\n",
        "    axes[2, idx].set_ylabel(var)\n",
        "    axes[2, idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Log-transformed distributions for better visualization of skewed data\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Log-Transformed Distributions (for Skewed Variables)', fontsize=16)\n",
        "\n",
        "for idx, var in enumerate(variables):\n",
        "    log_data = np.log1p(df[var][df[var] > 0])  # log1p to handle zeros\n",
        "    axes[idx].hist(log_data, bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "    axes[idx].set_title(f'Log({var}) Distribution', fontweight='bold')\n",
        "    axes[idx].set_xlabel(f'Log({var})')\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Correlation Analysis\n",
        "\n",
        "Heatmap showing relationships between numeric variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Correlation Heatmap\n",
        "print(\"=\" * 80)\n",
        "print(\"3. CORRELATION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_vars = ['Quantity', 'UnitPrice', 'TotalPrice', 'Year', 'Month', 'DayOfWeek', 'Hour']\n",
        "corr_matrix = df[corr_vars].corr()\n",
        "\n",
        "# Create heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.3f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
        "ax.set_title('Correlation Heatmap of Key Variables', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print correlation values\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(corr_matrix.round(3))\n",
        "\n",
        "# Identify strong correlations\n",
        "print(\"\\nStrong Correlations (|r| > 0.5):\")\n",
        "strong_corr = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "        corr_val = corr_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.5:\n",
        "            strong_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
        "            print(f\"  {corr_matrix.columns[i]} ↔ {corr_matrix.columns[j]}: {corr_val:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Missing Data Analysis\n",
        "\n",
        "Visual matrix showing missing data patterns across the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Missing Data Matrix\n",
        "print(\"=\" * 80)\n",
        "print(\"4. MISSING DATA ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate missing values\n",
        "missing_data = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing Count': df.isnull().sum(),\n",
        "    'Missing %': (df.isnull().sum() / len(df)) * 100\n",
        "})\n",
        "missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
        "\n",
        "print(\"\\nMissing Data Summary:\")\n",
        "if len(missing_data) > 0:\n",
        "    print(missing_data.to_string(index=False))\n",
        "else:\n",
        "    print(\"  No missing values found in cleaned dataset!\")\n",
        "\n",
        "# Visual missing data matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Missing data heatmap\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis', ax=axes[0])\n",
        "    axes[0].set_title('Missing Data Matrix (Yellow = Missing)', fontweight='bold')\n",
        "else:\n",
        "    axes[0].text(0.5, 0.5, 'No Missing Data', ha='center', va='center', \n",
        "                fontsize=16, transform=axes[0].transAxes)\n",
        "    axes[0].set_title('Missing Data Matrix', fontweight='bold')\n",
        "\n",
        "# Missing data bar chart\n",
        "if len(missing_data) > 0:\n",
        "    sns.barplot(data=missing_data, x='Missing %', y='Column', palette='Reds_r', ax=axes[1])\n",
        "    axes[1].set_title('Missing Data by Column (%)', fontweight='bold')\n",
        "    axes[1].set_xlabel('Missing Percentage')\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'No Missing Data', ha='center', va='center', \n",
        "                fontsize=16, transform=axes[1].transAxes)\n",
        "    axes[1].set_title('Missing Data by Column', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check raw data for comparison\n",
        "print(\"\\nMissing Data in Raw Dataset:\")\n",
        "raw_missing = pd.DataFrame({\n",
        "    'Column': df_raw.columns,\n",
        "    'Missing Count': df_raw.isnull().sum(),\n",
        "    'Missing %': (df_raw.isnull().sum() / len(df_raw)) * 100\n",
        "})\n",
        "print(raw_missing[raw_missing['Missing Count'] > 0].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Outlier Detection\n",
        "\n",
        "Using IQR (Interquartile Range) and Z-score methods to identify outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Outlier Detection: IQR and Z-score Methods\n",
        "print(\"=\" * 80)\n",
        "print(\"5. OUTLIER DETECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    \"\"\"Detect outliers using IQR method\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "def detect_outliers_zscore(data, column, threshold=3):\n",
        "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
        "    z_scores = np.abs(zscore(data[column].dropna()))\n",
        "    outliers = data[z_scores > threshold]\n",
        "    return outliers\n",
        "\n",
        "# Detect outliers for key variables\n",
        "outlier_results = {}\n",
        "variables = ['Quantity', 'UnitPrice', 'TotalPrice']\n",
        "\n",
        "for var in variables:\n",
        "    iqr_outliers, lower, upper = detect_outliers_iqr(df, var)\n",
        "    zscore_outliers = detect_outliers_zscore(df, var, threshold=3)\n",
        "    \n",
        "    outlier_results[var] = {\n",
        "        'IQR_count': len(iqr_outliers),\n",
        "        'IQR_percent': (len(iqr_outliers) / len(df)) * 100,\n",
        "        'Zscore_count': len(zscore_outliers),\n",
        "        'Zscore_percent': (len(zscore_outliers) / len(df)) * 100,\n",
        "        'IQR_lower': lower,\n",
        "        'IQR_upper': upper\n",
        "    }\n",
        "\n",
        "# Create summary table\n",
        "outlier_summary = pd.DataFrame({\n",
        "    'Variable': variables,\n",
        "    'IQR Outliers': [outlier_results[v]['IQR_count'] for v in variables],\n",
        "    'IQR %': [outlier_results[v]['IQR_percent'] for v in variables],\n",
        "    'Z-score Outliers': [outlier_results[v]['Zscore_count'] for v in variables],\n",
        "    'Z-score %': [outlier_results[v]['Zscore_percent'] for v in variables],\n",
        "    'IQR Lower Bound': [outlier_results[v]['IQR_lower'] for v in variables],\n",
        "    'IQR Upper Bound': [outlier_results[v]['IQR_upper'] for v in variables]\n",
        "})\n",
        "\n",
        "print(\"\\nOutlier Detection Summary:\")\n",
        "print(outlier_summary.to_string(index=False))\n",
        "\n",
        "# Visualize outliers\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Outlier Detection: IQR and Z-score Methods', fontsize=16, y=1.02)\n",
        "\n",
        "for idx, var in enumerate(variables):\n",
        "    # IQR Method\n",
        "    iqr_outliers, lower, upper = detect_outliers_iqr(df, var)\n",
        "    axes[0, idx].scatter(df.index, df[var], alpha=0.5, s=10, color='blue', label='Normal')\n",
        "    axes[0, idx].scatter(iqr_outliers.index, iqr_outliers[var], alpha=0.7, s=20, \n",
        "                         color='red', label='IQR Outliers')\n",
        "    axes[0, idx].axhline(y=lower, color='orange', linestyle='--', label='Lower Bound')\n",
        "    axes[0, idx].axhline(y=upper, color='orange', linestyle='--', label='Upper Bound')\n",
        "    axes[0, idx].set_title(f'{var} - IQR Method\\n({len(iqr_outliers)} outliers)', fontweight='bold')\n",
        "    axes[0, idx].set_xlabel('Index')\n",
        "    axes[0, idx].set_ylabel(var)\n",
        "    axes[0, idx].legend()\n",
        "    axes[0, idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Z-score Method\n",
        "    zscore_outliers = detect_outliers_zscore(df, var, threshold=3)\n",
        "    z_scores = np.abs(zscore(df[var].dropna()))\n",
        "    axes[1, idx].scatter(range(len(z_scores)), z_scores, alpha=0.5, s=10, color='blue', label='Normal')\n",
        "    outlier_indices = np.where(z_scores > 3)[0]\n",
        "    if len(outlier_indices) > 0:\n",
        "        axes[1, idx].scatter(outlier_indices, z_scores[outlier_indices], alpha=0.7, s=20,\n",
        "                            color='red', label='Z-score Outliers')\n",
        "    axes[1, idx].axhline(y=3, color='orange', linestyle='--', label='Threshold (Z=3)')\n",
        "    axes[1, idx].set_title(f'{var} - Z-score Method\\n({len(zscore_outliers)} outliers)', fontweight='bold')\n",
        "    axes[1, idx].set_xlabel('Index')\n",
        "    axes[1, idx].set_ylabel('Absolute Z-score')\n",
        "    axes[1, idx].legend()\n",
        "    axes[1, idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Time-Trend Analysis and Market Volatility\n",
        "print(\"=\" * 80)\n",
        "print(\"6. TIME-TREND ANALYSIS & MARKET VOLATILITY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Aggregate by date\n",
        "daily_sales = df.groupby(df['InvoiceDate'].dt.date).agg({\n",
        "    'TotalPrice': ['sum', 'mean', 'count'],\n",
        "    'Quantity': 'sum',\n",
        "    'InvoiceNo': 'nunique'\n",
        "}).reset_index()\n",
        "daily_sales.columns = ['Date', 'DailyRevenue', 'AvgTransactionValue', 'TransactionCount', 'TotalQuantity', 'UniqueInvoices']\n",
        "daily_sales['Date'] = pd.to_datetime(daily_sales['Date'])\n",
        "\n",
        "# Calculate rolling statistics for volatility\n",
        "daily_sales['Revenue_7d_MA'] = daily_sales['DailyRevenue'].rolling(window=7, center=True).mean()\n",
        "daily_sales['Revenue_30d_MA'] = daily_sales['DailyRevenue'].rolling(window=30, center=True).mean()\n",
        "daily_sales['Revenue_Std'] = daily_sales['DailyRevenue'].rolling(window=7, center=True).std()\n",
        "daily_sales['Volatility'] = daily_sales['Revenue_Std'] / daily_sales['Revenue_7d_MA'] * 100  # Coefficient of variation\n",
        "\n",
        "# Time series plots\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 16))\n",
        "fig.suptitle('Time-Trend Analysis: Revenue, Transactions, and Volatility', fontsize=16, y=0.995)\n",
        "\n",
        "# 1. Daily Revenue with moving averages\n",
        "axes[0].plot(daily_sales['Date'], daily_sales['DailyRevenue'], alpha=0.6, color='steelblue', label='Daily Revenue', linewidth=1)\n",
        "axes[0].plot(daily_sales['Date'], daily_sales['Revenue_7d_MA'], color='orange', label='7-day MA', linewidth=2)\n",
        "axes[0].plot(daily_sales['Date'], daily_sales['Revenue_30d_MA'], color='red', label='30-day MA', linewidth=2)\n",
        "axes[0].fill_between(daily_sales['Date'], \n",
        "                     daily_sales['Revenue_7d_MA'] - daily_sales['Revenue_Std'],\n",
        "                     daily_sales['Revenue_7d_MA'] + daily_sales['Revenue_Std'],\n",
        "                     alpha=0.2, color='orange', label='±1 Std Dev')\n",
        "axes[0].set_title('Daily Revenue Trend with Moving Averages', fontweight='bold')\n",
        "axes[0].set_ylabel('Revenue (£)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Transaction Count\n",
        "axes[1].plot(daily_sales['Date'], daily_sales['TransactionCount'], alpha=0.7, color='green', linewidth=1.5)\n",
        "axes[1].set_title('Daily Transaction Count', fontweight='bold')\n",
        "axes[1].set_ylabel('Number of Transactions')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Volatility (Coefficient of Variation)\n",
        "axes[2].plot(daily_sales['Date'], daily_sales['Volatility'], alpha=0.7, color='purple', linewidth=1.5)\n",
        "axes[2].axhline(y=daily_sales['Volatility'].mean(), color='red', linestyle='--', label=f'Mean: {daily_sales[\"Volatility\"].mean():.2f}%')\n",
        "axes[2].set_title('Market Volatility (7-day Rolling CV%)', fontweight='bold')\n",
        "axes[2].set_ylabel('Volatility (%)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Monthly aggregation\n",
        "monthly_sales = df.groupby('MonthYear').agg({\n",
        "    'TotalPrice': ['sum', 'mean'],\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'Quantity': 'sum'\n",
        "}).reset_index()\n",
        "monthly_sales.columns = ['Month', 'MonthlyRevenue', 'AvgTransaction', 'UniqueInvoices', 'TotalQuantity']\n",
        "monthly_sales['Month'] = pd.to_datetime(monthly_sales['Month'])\n",
        "\n",
        "axes[3].bar(monthly_sales['Month'], monthly_sales['MonthlyRevenue'], alpha=0.7, color='teal', edgecolor='black')\n",
        "axes[3].set_title('Monthly Revenue', fontweight='bold')\n",
        "axes[3].set_ylabel('Revenue (£)')\n",
        "axes[3].set_xlabel('Month')\n",
        "axes[3].tick_params(axis='x', rotation=45)\n",
        "axes[3].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Hourly and Day-of-Week patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "fig.suptitle('Temporal Patterns: Hourly and Weekly Trends', fontsize=16, y=0.995)\n",
        "\n",
        "# Hourly revenue\n",
        "hourly_revenue = df.groupby('Hour')['TotalPrice'].sum().sort_index()\n",
        "axes[0, 0].bar(hourly_revenue.index, hourly_revenue.values, alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[0, 0].set_title('Revenue by Hour of Day', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Hour')\n",
        "axes[0, 0].set_ylabel('Revenue (£)')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Day of week revenue\n",
        "dow_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\n",
        "dow_revenue = df.groupby('DayOfWeek')['TotalPrice'].sum().sort_index()\n",
        "axes[0, 1].bar(range(len(dow_revenue)), dow_revenue.values, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 1].set_xticks(range(len(dow_revenue)))\n",
        "axes[0, 1].set_xticklabels([dow_map[i] for i in dow_revenue.index])\n",
        "axes[0, 1].set_title('Revenue by Day of Week', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Revenue (£)')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Monthly revenue trend\n",
        "axes[1, 0].plot(monthly_sales['Month'], monthly_sales['MonthlyRevenue'], marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
        "axes[1, 0].set_title('Monthly Revenue Trend', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Revenue (£)')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Transaction count by day of week\n",
        "dow_transactions = df.groupby('DayOfWeek')['InvoiceNo'].nunique().sort_index()\n",
        "axes[1, 1].bar(range(len(dow_transactions)), dow_transactions.values, alpha=0.7, color='gold', edgecolor='black')\n",
        "axes[1, 1].set_xticks(range(len(dow_transactions)))\n",
        "axes[1, 1].set_xticklabels([dow_map[i] for i in dow_transactions.index])\n",
        "axes[1, 1].set_title('Unique Invoices by Day of Week', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Number of Invoices')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Volatility statistics\n",
        "print(\"\\nVolatility Statistics:\")\n",
        "print(f\"  Mean Volatility: {daily_sales['Volatility'].mean():.2f}%\")\n",
        "print(f\"  Max Volatility: {daily_sales['Volatility'].max():.2f}%\")\n",
        "print(f\"  Min Volatility: {daily_sales['Volatility'].min():.2f}%\")\n",
        "print(f\"  Std Dev of Volatility: {daily_sales['Volatility'].std():.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Customer-Level Analysis: RFM Segmentation\n",
        "print(\"=\" * 80)\n",
        "print(\"7. CUSTOMER-LEVEL ANALYSIS (RFM SEGMENTATION)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate RFM metrics\n",
        "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',  # Frequency\n",
        "    'TotalPrice': 'sum'  # Monetary\n",
        "}).reset_index()\n",
        "\n",
        "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Create RFM scores (1-5 scale, where 5 is best)\n",
        "rfm['R_Score'] = pd.qcut(rfm['Recency'], q=5, labels=[5, 4, 3, 2, 1], duplicates='drop')\n",
        "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
        "rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n",
        "\n",
        "# Convert to numeric\n",
        "rfm['R_Score'] = rfm['R_Score'].astype(int)\n",
        "rfm['F_Score'] = rfm['F_Score'].astype(int)\n",
        "rfm['M_Score'] = rfm['M_Score'].astype(int)\n",
        "\n",
        "# Create RFM segment\n",
        "rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
        "\n",
        "# Define customer segments\n",
        "def segment_customer(row):\n",
        "    if row['R_Score'] >= 4 and row['F_Score'] >= 4 and row['M_Score'] >= 4:\n",
        "        return 'Champions'\n",
        "    elif row['R_Score'] >= 3 and row['F_Score'] >= 3 and row['M_Score'] >= 3:\n",
        "        return 'Loyal Customers'\n",
        "    elif row['R_Score'] >= 4 and row['F_Score'] <= 2:\n",
        "        return 'New Customers'\n",
        "    elif row['R_Score'] <= 2 and row['F_Score'] >= 3:\n",
        "        return 'At Risk'\n",
        "    elif row['R_Score'] <= 2 and row['F_Score'] <= 2:\n",
        "        return 'Lost Customers'\n",
        "    else:\n",
        "        return 'Regular'\n",
        "\n",
        "rfm['Segment'] = rfm.apply(segment_customer, axis=1)\n",
        "\n",
        "# RFM Summary Statistics\n",
        "print(\"\\nRFM Summary Statistics:\")\n",
        "print(rfm[['Recency', 'Frequency', 'Monetary']].describe())\n",
        "\n",
        "# Customer segment distribution\n",
        "print(\"\\nCustomer Segment Distribution:\")\n",
        "segment_dist = rfm['Segment'].value_counts()\n",
        "print(segment_dist)\n",
        "print(f\"\\nSegment Revenue Contribution:\")\n",
        "segment_revenue = rfm.groupby('Segment')['Monetary'].sum().sort_values(ascending=False)\n",
        "for seg, rev in segment_revenue.items():\n",
        "    print(f\"  {seg}: £{rev:,.2f} ({rev/rfm['Monetary'].sum()*100:.2f}%)\")\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Customer RFM Analysis', fontsize=16, y=0.995)\n",
        "\n",
        "# RFM Score distributions\n",
        "axes[0, 0].hist(rfm['R_Score'], bins=5, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0, 0].set_title('Recency Score Distribution', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Recency Score')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[0, 1].hist(rfm['F_Score'], bins=5, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[0, 1].set_title('Frequency Score Distribution', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Frequency Score')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[0, 2].hist(rfm['M_Score'], bins=5, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[0, 2].set_title('Monetary Score Distribution', fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Monetary Score')\n",
        "axes[0, 2].set_ylabel('Count')\n",
        "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Segment distribution\n",
        "axes[1, 0].barh(segment_dist.index, segment_dist.values, alpha=0.7, color='teal', edgecolor='black')\n",
        "axes[1, 0].set_title('Customer Segment Count', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Number of Customers')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Segment revenue\n",
        "axes[1, 1].barh(segment_revenue.index, segment_revenue.values, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1, 1].set_title('Revenue by Segment', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Revenue (£)')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# RFM scatter plot (Frequency vs Monetary, colored by Recency)\n",
        "scatter = axes[1, 2].scatter(rfm['Frequency'], rfm['Monetary'], c=rfm['Recency'], \n",
        "                            cmap='RdYlGn_r', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "axes[1, 2].set_title('Frequency vs Monetary (colored by Recency)', fontweight='bold')\n",
        "axes[1, 2].set_xlabel('Frequency')\n",
        "axes[1, 2].set_ylabel('Monetary Value (£)')\n",
        "axes[1, 2].set_xscale('log')\n",
        "axes[1, 2].set_yscale('log')\n",
        "plt.colorbar(scatter, ax=axes[1, 2], label='Recency (days)')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional customer statistics\n",
        "print(\"\\nCustomer Statistics:\")\n",
        "print(f\"  Total Customers: {len(rfm):,}\")\n",
        "print(f\"  Average Recency: {rfm['Recency'].mean():.1f} days\")\n",
        "print(f\"  Average Frequency: {rfm['Frequency'].mean():.2f} transactions\")\n",
        "print(f\"  Average Monetary Value: £{rfm['Monetary'].mean():,.2f}\")\n",
        "print(f\"  Median Monetary Value: £{rfm['Monetary'].median():,.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Product/Category-Level Insights\n",
        "print(\"=\" * 80)\n",
        "print(\"8. PRODUCT/CATEGORY-LEVEL INSIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Product performance metrics\n",
        "product_stats = df.groupby('StockCode').agg({\n",
        "    'Quantity': ['sum', 'mean'],\n",
        "    'TotalPrice': ['sum', 'mean'],\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'CustomerID': 'nunique',\n",
        "    'Description': 'first'\n",
        "}).reset_index()\n",
        "product_stats.columns = ['StockCode', 'TotalQuantity', 'AvgQuantity', 'TotalRevenue', \n",
        "                         'AvgPrice', 'TransactionCount', 'UniqueCustomers', 'Description']\n",
        "\n",
        "# Top products\n",
        "print(\"\\nTop 10 Products by Revenue:\")\n",
        "top_revenue = product_stats.nlargest(10, 'TotalRevenue')[['Description', 'TotalRevenue', 'TotalQuantity', 'TransactionCount']]\n",
        "print(top_revenue.to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 10 Products by Quantity Sold:\")\n",
        "top_quantity = product_stats.nlargest(10, 'TotalQuantity')[['Description', 'TotalQuantity', 'TotalRevenue', 'TransactionCount']]\n",
        "print(top_quantity.to_string(index=False))\n",
        "\n",
        "print(\"\\nTop 10 Products by Transaction Frequency:\")\n",
        "top_freq = product_stats.nlargest(10, 'TransactionCount')[['Description', 'TransactionCount', 'TotalRevenue', 'UniqueCustomers']]\n",
        "print(top_freq.to_string(index=False))\n",
        "\n",
        "# Product diversity metrics\n",
        "print(\"\\nProduct Diversity Metrics:\")\n",
        "print(f\"  Total Unique Products: {df['StockCode'].nunique():,}\")\n",
        "print(f\"  Products with >100 transactions: {len(product_stats[product_stats['TransactionCount'] > 100]):,}\")\n",
        "print(f\"  Products with >£10,000 revenue: {len(product_stats[product_stats['TotalRevenue'] > 10000]):,}\")\n",
        "print(f\"  Average revenue per product: £{product_stats['TotalRevenue'].mean():,.2f}\")\n",
        "print(f\"  Median revenue per product: £{product_stats['TotalRevenue'].median():,.2f}\")\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Product Performance Analysis', fontsize=16, y=0.995)\n",
        "\n",
        "# Top 15 products by revenue\n",
        "top15_revenue = product_stats.nlargest(15, 'TotalRevenue')\n",
        "axes[0, 0].barh(range(len(top15_revenue)), top15_revenue['TotalRevenue'], alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0, 0].set_yticks(range(len(top15_revenue)))\n",
        "axes[0, 0].set_yticklabels([desc[:40] + '...' if len(desc) > 40 else desc for desc in top15_revenue['Description']], fontsize=8)\n",
        "axes[0, 0].set_title('Top 15 Products by Revenue', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Revenue (£)')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Revenue distribution (log scale)\n",
        "axes[0, 1].hist(np.log1p(product_stats['TotalRevenue']), bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[0, 1].set_title('Product Revenue Distribution (Log Scale)', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Log(Revenue)')\n",
        "axes[0, 1].set_ylabel('Number of Products')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Quantity vs Revenue scatter\n",
        "axes[1, 0].scatter(product_stats['TotalQuantity'], product_stats['TotalRevenue'], \n",
        "                   alpha=0.5, s=30, color='coral', edgecolors='black', linewidth=0.5)\n",
        "axes[1, 0].set_title('Quantity vs Revenue', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Total Quantity Sold')\n",
        "axes[1, 0].set_ylabel('Total Revenue (£)')\n",
        "axes[1, 0].set_xscale('log')\n",
        "axes[1, 0].set_yscale('log')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Transaction frequency distribution\n",
        "axes[1, 1].hist(product_stats['TransactionCount'], bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
        "axes[1, 1].set_title('Product Transaction Frequency Distribution', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Number of Transactions')\n",
        "axes[1, 1].set_ylabel('Number of Products')\n",
        "axes[1, 1].set_xscale('log')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Price analysis\n",
        "print(\"\\nPrice Analysis:\")\n",
        "print(f\"  Average Unit Price: £{df['UnitPrice'].mean():.2f}\")\n",
        "print(f\"  Median Unit Price: £{df['UnitPrice'].median():.2f}\")\n",
        "print(f\"  Price Range: £{df['UnitPrice'].min():.2f} - £{df['UnitPrice'].max():,.2f}\")\n",
        "print(f\"  Products under £10: {len(df[df['UnitPrice'] < 10])/len(df)*100:.1f}% of transactions\")\n",
        "print(f\"  Products £10-50: {len(df[(df['UnitPrice'] >= 10) & (df['UnitPrice'] < 50)])/len(df)*100:.1f}% of transactions\")\n",
        "print(f\"  Products over £50: {len(df[df['UnitPrice'] >= 50])/len(df)*100:.1f}% of transactions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Deep Insights: Market Volatility, Seasonal Patterns, and Geographical Analysis\n",
        "\n",
        "Comprehensive analysis of market behavior patterns including volatility observations, seasonal trends, and geographical demand patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Deep Insights: Volatility, Seasonal, and Geographical Patterns\n",
        "print(\"=\" * 80)\n",
        "print(\"9. DEEP INSIGHTS: VOLATILITY, SEASONAL, & GEOGRAPHICAL ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 9.1 Market Volatility Deep Dive\n",
        "print(\"\\n9.1 MARKET VOLATILITY OBSERVATIONS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate volatility metrics\n",
        "daily_sales['DailyChange'] = daily_sales['DailyRevenue'].pct_change() * 100\n",
        "daily_sales['DailyChangeAbs'] = daily_sales['DailyChange'].abs()\n",
        "\n",
        "# Identify high volatility periods\n",
        "high_vol_threshold = daily_sales['Volatility'].quantile(0.75)\n",
        "high_vol_days = daily_sales[daily_sales['Volatility'] > high_vol_threshold]\n",
        "\n",
        "print(f\"\\nVolatility Metrics:\")\n",
        "print(f\"  High Volatility Threshold (75th percentile): {high_vol_threshold:.2f}%\")\n",
        "print(f\"  Days with High Volatility: {len(high_vol_days)} ({len(high_vol_days)/len(daily_sales)*100:.1f}%)\")\n",
        "print(f\"  Average Daily Revenue Change: {daily_sales['DailyChange'].mean():.2f}%\")\n",
        "print(f\"  Average Absolute Daily Change: {daily_sales['DailyChangeAbs'].mean():.2f}%\")\n",
        "print(f\"  Max Daily Revenue Increase: {daily_sales['DailyChange'].max():.2f}%\")\n",
        "print(f\"  Max Daily Revenue Decrease: {daily_sales['DailyChange'].min():.2f}%\")\n",
        "\n",
        "# 9.2 Seasonal Patterns\n",
        "print(\"\\n9.2 SEASONAL PATTERNS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Monthly seasonality\n",
        "monthly_agg = df.groupby('Month').agg({\n",
        "    'TotalPrice': ['sum', 'mean', 'count'],\n",
        "    'InvoiceNo': 'nunique'\n",
        "}).reset_index()\n",
        "monthly_agg.columns = ['Month', 'MonthlyRevenue', 'AvgTransaction', 'TransactionCount', 'UniqueInvoices']\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "monthly_agg['MonthName'] = monthly_agg['Month'].apply(lambda x: month_names[x-1] if x <= 12 else 'Unknown')\n",
        "\n",
        "print(\"\\nMonthly Revenue Pattern:\")\n",
        "for _, row in monthly_agg.iterrows():\n",
        "    print(f\"  {row['MonthName']}: £{row['MonthlyRevenue']:,.2f} ({row['TransactionCount']:,} transactions)\")\n",
        "\n",
        "# Identify peak months\n",
        "peak_month = monthly_agg.loc[monthly_agg['MonthlyRevenue'].idxmax()]\n",
        "print(f\"\\nPeak Sales Month: {peak_month['MonthName']} (£{peak_month['MonthlyRevenue']:,.2f})\")\n",
        "print(f\"Lowest Sales Month: {monthly_agg.loc[monthly_agg['MonthlyRevenue'].idxmin(), 'MonthName']} (£{monthly_agg['MonthlyRevenue'].min():,.2f})\")\n",
        "\n",
        "# Day of week seasonality\n",
        "dow_seasonality = df.groupby('DayOfWeek').agg({\n",
        "    'TotalPrice': 'sum',\n",
        "    'InvoiceNo': 'nunique'\n",
        "}).reset_index()\n",
        "dow_seasonality['DayName'] = dow_seasonality['DayOfWeek'].map(dow_map)\n",
        "print(\"\\nDay of Week Pattern:\")\n",
        "for _, row in dow_seasonality.iterrows():\n",
        "    print(f\"  {row['DayName']}: £{row['TotalPrice']:,.2f} ({row['InvoiceNo']:,} invoices)\")\n",
        "\n",
        "# 9.3 Geographical Demand Patterns\n",
        "print(\"\\n9.3 GEOGRAPHICAL DEMAND PATTERNS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "country_stats = df.groupby('Country').agg({\n",
        "    'TotalPrice': ['sum', 'mean'],\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'CustomerID': 'nunique',\n",
        "    'Quantity': 'sum'\n",
        "}).reset_index()\n",
        "country_stats.columns = ['Country', 'TotalRevenue', 'AvgTransaction', 'UniqueInvoices', 'UniqueCustomers', 'TotalQuantity']\n",
        "country_stats = country_stats.sort_values('TotalRevenue', ascending=False)\n",
        "\n",
        "print(f\"\\nTotal Countries: {len(country_stats)}\")\n",
        "print(f\"\\nTop 10 Countries by Revenue:\")\n",
        "print(country_stats.head(10)[['Country', 'TotalRevenue', 'UniqueCustomers', 'UniqueInvoices']].to_string(index=False))\n",
        "\n",
        "print(f\"\\nGeographical Concentration:\")\n",
        "top5_revenue = country_stats.head(5)['TotalRevenue'].sum()\n",
        "print(f\"  Top 5 countries account for {top5_revenue/df['TotalPrice'].sum()*100:.1f}% of total revenue\")\n",
        "print(f\"  Top country (UK) accounts for {country_stats.iloc[0]['TotalRevenue']/df['TotalPrice'].sum()*100:.1f}% of total revenue\")\n",
        "\n",
        "# Visualizations for deep insights\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "fig.suptitle('Deep Insights: Volatility, Seasonal, and Geographical Patterns', fontsize=16, y=0.995)\n",
        "\n",
        "# Volatility over time\n",
        "axes[0, 0].plot(daily_sales['Date'], daily_sales['Volatility'], alpha=0.7, color='red', linewidth=1.5)\n",
        "axes[0, 0].axhline(y=high_vol_threshold, color='orange', linestyle='--', label=f'High Vol Threshold ({high_vol_threshold:.1f}%)')\n",
        "axes[0, 0].fill_between(daily_sales['Date'], 0, daily_sales['Volatility'], \n",
        "                        where=(daily_sales['Volatility'] > high_vol_threshold), \n",
        "                        alpha=0.3, color='red', label='High Volatility Periods')\n",
        "axes[0, 0].set_title('Market Volatility Over Time', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Volatility (%)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Daily revenue changes\n",
        "axes[0, 1].plot(daily_sales['Date'], daily_sales['DailyChange'], alpha=0.6, color='steelblue', linewidth=1)\n",
        "axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[0, 1].set_title('Daily Revenue Change (%)', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Percentage Change')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Monthly seasonality\n",
        "axes[1, 0].bar(range(len(monthly_agg)), monthly_agg['MonthlyRevenue'], alpha=0.7, color='teal', edgecolor='black')\n",
        "axes[1, 0].set_xticks(range(len(monthly_agg)))\n",
        "axes[1, 0].set_xticklabels(monthly_agg['MonthName'], rotation=45)\n",
        "axes[1, 0].set_title('Monthly Revenue Seasonality', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Revenue (£)')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Day of week pattern\n",
        "axes[1, 1].bar(range(len(dow_seasonality)), dow_seasonality['TotalPrice'], alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[1, 1].set_xticks(range(len(dow_seasonality)))\n",
        "axes[1, 1].set_xticklabels(dow_seasonality['DayName'])\n",
        "axes[1, 1].set_title('Day of Week Revenue Pattern', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Revenue (£)')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Top 15 countries by revenue\n",
        "top15_countries = country_stats.head(15)\n",
        "axes[2, 0].barh(range(len(top15_countries)), top15_countries['TotalRevenue'], alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[2, 0].set_yticks(range(len(top15_countries)))\n",
        "axes[2, 0].set_yticklabels(top15_countries['Country'], fontsize=9)\n",
        "axes[2, 0].set_title('Top 15 Countries by Revenue', fontweight='bold')\n",
        "axes[2, 0].set_xlabel('Revenue (£)')\n",
        "axes[2, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Revenue concentration (Pareto chart)\n",
        "country_stats_sorted = country_stats.sort_values('TotalRevenue', ascending=False)\n",
        "cumulative_pct = (country_stats_sorted['TotalRevenue'].cumsum() / country_stats_sorted['TotalRevenue'].sum() * 100)\n",
        "axes[2, 1].plot(range(1, min(21, len(country_stats_sorted)+1)), cumulative_pct[:20], \n",
        "               marker='o', linewidth=2, markersize=6, color='darkgreen')\n",
        "axes[2, 1].axhline(y=80, color='red', linestyle='--', label='80% Line')\n",
        "axes[2, 1].set_title('Revenue Concentration (Pareto)', fontweight='bold')\n",
        "axes[2, 1].set_xlabel('Number of Countries')\n",
        "axes[2, 1].set_ylabel('Cumulative Revenue %')\n",
        "axes[2, 1].legend()\n",
        "axes[2, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary insights\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY INSIGHTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n1. Market Volatility:\")\n",
        "print(f\"   - Average volatility: {daily_sales['Volatility'].mean():.2f}%\")\n",
        "print(f\"   - {len(high_vol_days)} high volatility days identified\")\n",
        "print(f\"   - Revenue shows {'high' if daily_sales['Volatility'].mean() > 30 else 'moderate' if daily_sales['Volatility'].mean() > 15 else 'low'} volatility\")\n",
        "\n",
        "print(f\"\\n2. Seasonal Patterns:\")\n",
        "print(f\"   - Peak month: {peak_month['MonthName']}\")\n",
        "print(f\"   - Strongest day: {dow_seasonality.loc[dow_seasonality['TotalPrice'].idxmax(), 'DayName']}\")\n",
        "print(f\"   - Revenue variation across months: {(monthly_agg['MonthlyRevenue'].max() - monthly_agg['MonthlyRevenue'].min())/monthly_agg['MonthlyRevenue'].mean()*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n3. Geographical Patterns:\")\n",
        "print(f\"   - {len(country_stats)} countries represented\")\n",
        "print(f\"   - Top country (UK) dominance: {country_stats.iloc[0]['TotalRevenue']/df['TotalPrice'].sum()*100:.1f}%\")\n",
        "print(f\"   - Top 5 countries: {top5_revenue/df['TotalPrice'].sum()*100:.1f}% of revenue\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
