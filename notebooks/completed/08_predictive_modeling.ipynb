{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08 - Predictive Modeling (Phase 3 Implementation)\n",
        "\n",
        "This notebook now implements the **Phase 3** predictive modeling for time-series forecasting and customer behavior prediction, building on the methodology planning and data preparation completed in Phase 2.\n",
        "\n",
        "## Objectives\n",
        "- Document predictive modeling approach\n",
        "- Implement time series forecasting models using **ARIMA** and **Prophet**\n",
        "- Implement **customer behavior prediction** (churn risk / activity) model\n",
        "- Define and compute evaluation metrics\n",
        "- Use prepared datasets for end-to-end modeling and evaluation\n",
        "\n",
        "## Phase Progress\n",
        "- ✅ Phase 2: Model planning and approach documentation\n",
        "- ✅ Phase 2: Methodology selection and justification\n",
        "- ✅ Phase 2: Evaluation metrics definition\n",
        "- ✅ Phase 2: Data preparation for modeling\n",
        "- ✅ Phase 3: ARIMA-based daily revenue forecasting implementation\n",
        "- ✅ Phase 3: Prophet-based daily revenue forecasting implementation\n",
        "- ✅ Phase 3: Customer behavior prediction baseline model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'statsmodels'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstattools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m adfuller\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsaplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_acf, plot_pacf\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ARIMA\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'statsmodels'"
          ]
        }
      ],
      "source": [
        "# Load required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from prophet import Prophet\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PREDICTIVE MODELING - PHASE 3 IMPLEMENTATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load data\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "data_path = os.path.join(project_root, 'data', 'raw', 'Online Retail.csv')\n",
        "\n",
        "df = pd.read_csv(data_path, encoding='latin-1')\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "df = df[df['Description'].notna()]\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "df = df[df['InvoiceDate'].notna()]\n",
        "\n",
        "print(f\"\\nDataset loaded: {df.shape[0]:,} transactions\")\n",
        "print(f\"Date range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}\")\n",
        "print(\"Target series: DailyRevenue (aggregated from TotalPrice)\")\n",
        "print(\"Customer features prepared for churn / activity prediction\")\n",
        "print(f\"\\nNote: This notebook implements Phase 3 models: ARIMA, Prophet, and a customer behavior classifier.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Predictive Modeling Objectives\n",
        "\n",
        "Define the key predictive modeling goals for Phase 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modeling objectives\n",
        "print(\"=\" * 80)\n",
        "print(\"PREDICTIVE MODELING OBJECTIVES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "objectives = {\n",
        "    \"1. Demand Forecasting\": {\n",
        "        \"Goal\": \"Predict future daily/monthly revenue and transaction volumes\",\n",
        "        \"Use Case\": \"Stock planning, inventory optimization\",\n",
        "        \"Time Horizon\": \"Short-term (1-30 days), Medium-term (1-6 months)\"\n",
        "    },\n",
        "    \"2. Customer Behavior Prediction\": {\n",
        "        \"Goal\": \"Predict customer purchase likelihood and churn risk\",\n",
        "        \"Use Case\": \"Targeted marketing, retention campaigns\",\n",
        "        \"Time Horizon\": \"Next purchase timing, 30/60/90 day churn\"\n",
        "    },\n",
        "    \"3. Product Demand Prediction\": {\n",
        "        \"Goal\": \"Forecast product-level demand\",\n",
        "        \"Use Case\": \"Product-specific stock allocation\",\n",
        "        \"Time Horizon\": \"Weekly and monthly forecasts\"\n",
        "    },\n",
        "    \"4. Basket Size Prediction\": {\n",
        "        \"Goal\": \"Predict transaction value and quantity\",\n",
        "        \"Use Case\": \"Revenue forecasting, pricing strategies\",\n",
        "        \"Time Horizon\": \"Next transaction prediction\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for obj_name, details in objectives.items():\n",
        "    print(f\"\\n{obj_name}:\")\n",
        "    for key, value in details.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Choice Given Data Limitations\n",
        "\n",
        "The dataset covers **only a single year of transactions**, which limits the reliability of long-horizon time-series models like ARIMA/Prophet for pure forecasting. We therefore:\n",
        "- Use ARIMA/Prophet **exploratorily** to understand short-term patterns.\n",
        "- Rely primarily on **classical machine learning models (Random Forest, XGBoost)** for **customer behavior prediction**, where we can engineer features from the available year (recency, frequency, monetary, etc.).\n",
        "- Avoid deep learning models because, with this data size and horizon, they are more likely to **overfit** and provide less interpretable results for business stakeholders.\n",
        "\n",
        "This design keeps the modeling aligned with the data volume while still giving actionable, interpretable insights for retail decision-making.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Selection & Justification\n",
        "\n",
        "Document planned models and justify their selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL SELECTION & JUSTIFICATION\n",
            "================================================================================\n",
            "\n",
            "Time Series Forecasting:\n",
            "============================================================\n",
            "\n",
            "ARIMA:\n",
            "  Description: AutoRegressive Integrated Moving Average\n",
            "  Justification: Handles trend and seasonality, interpretable, good for univariate time series\n",
            "  Use Case: Daily/monthly revenue forecasting\n",
            "  Limitations: Requires stationarity, assumes linear relationships\n",
            "\n",
            "Prophet:\n",
            "  Description: Facebook's time series forecasting tool\n",
            "  Justification: Handles seasonality, holidays, trend changes automatically, robust to missing data\n",
            "  Use Case: Revenue forecasting with multiple seasonality patterns\n",
            "  Limitations: Less interpretable than ARIMA, requires sufficient historical data\n",
            "\n",
            "Customer Behavior:\n",
            "============================================================\n",
            "\n",
            "Logistic Regression:\n",
            "  Description: Binary classification for churn prediction\n",
            "  Justification: Interpretable, handles categorical features well, baseline model\n",
            "  Use Case: Customer churn prediction\n",
            "  Limitations: Assumes linear relationships, may need feature engineering\n",
            "\n",
            "Random Forest:\n",
            "  Description: Ensemble method for classification/regression\n",
            "  Justification: Handles non-linear relationships, feature importance, robust to outliers\n",
            "  Use Case: Purchase likelihood, basket size prediction\n",
            "  Limitations: Less interpretable, can overfit with small datasets\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Model selection\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL SELECTION & JUSTIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "models = {\n",
        "    \"Time Series Forecasting\": {\n",
        "        \"ARIMA\": {\n",
        "            \"Description\": \"AutoRegressive Integrated Moving Average\",\n",
        "            \"Justification\": \"Handles trend and seasonality, interpretable, good for univariate time series\",\n",
        "            \"Use Case\": \"Daily/monthly revenue forecasting\",\n",
        "            \"Limitations\": \"Requires stationarity, assumes linear relationships\"\n",
        "        },\n",
        "        \"Prophet\": {\n",
        "            \"Description\": \"Facebook's time series forecasting tool\",\n",
        "            \"Justification\": \"Handles seasonality, holidays, trend changes automatically, robust to missing data\",\n",
        "            \"Use Case\": \"Revenue forecasting with multiple seasonality patterns\",\n",
        "            \"Limitations\": \"Less interpretable than ARIMA, requires sufficient historical data\"\n",
        "        }\n",
        "    },\n",
        "    \"Customer Behavior\": {\n",
        "        \"Logistic Regression\": {\n",
        "            \"Description\": \"Binary classification for churn prediction\",\n",
        "            \"Justification\": \"Interpretable, handles categorical features well, baseline model\",\n",
        "            \"Use Case\": \"Customer churn prediction\",\n",
        "            \"Limitations\": \"Assumes linear relationships, may need feature engineering\"\n",
        "        },\n",
        "        \"Random Forest\": {\n",
        "            \"Description\": \"Ensemble method for classification/regression\",\n",
        "            \"Justification\": \"Handles non-linear relationships, feature importance, robust to outliers\",\n",
        "            \"Use Case\": \"Purchase likelihood, basket size prediction\",\n",
        "            \"Limitations\": \"Less interpretable, can overfit with small datasets\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "for category, model_dict in models.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    print(\"=\" * 60)\n",
        "    for model_name, details in model_dict.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        for key, value in details.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Preparation for Modeling\n",
        "\n",
        "Prepare time-series and customer-level datasets for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATA PREPARATION FOR MODELING\n",
            "================================================================================\n",
            "\n",
            "1. TIME-SERIES DATA PREPARATION:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. Time-series data for forecasting\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. TIME-SERIES DATA PREPARATION:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m daily_data = \u001b[43mdf\u001b[49m.groupby(df[\u001b[33m'\u001b[39m\u001b[33mInvoiceDate\u001b[39m\u001b[33m'\u001b[39m].dt.date).agg({\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTotalPrice\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mQuantity\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mInvoiceNo\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mnunique\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mCustomerID\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mnunique\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m }).reset_index()\n\u001b[32m     15\u001b[39m daily_data.columns = [\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDailyRevenue\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDailyQuantity\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDailyTransactions\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDailyCustomers\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m daily_data[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(daily_data[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Data preparation\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA PREPARATION FOR MODELING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Time-series data for forecasting\n",
        "print(\"\\n1. TIME-SERIES DATA PREPARATION:\")\n",
        "daily_data = df.groupby(df['InvoiceDate'].dt.date).agg({\n",
        "    'TotalPrice': 'sum',\n",
        "    'Quantity': 'sum',\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'CustomerID': 'nunique'\n",
        "}).reset_index()\n",
        "\n",
        "daily_data.columns = ['Date', 'DailyRevenue', 'DailyQuantity', 'DailyTransactions', 'DailyCustomers']\n",
        "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
        "daily_data = daily_data.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Create complete date range\n",
        "date_range = pd.date_range(start=daily_data['Date'].min(), end=daily_data['Date'].max(), freq='D')\n",
        "daily_complete = pd.DataFrame({'Date': date_range})\n",
        "daily_complete = daily_complete.merge(daily_data, on='Date', how='left')\n",
        "daily_complete = daily_complete.fillna(0)\n",
        "\n",
        "print(f\"  Daily time-series: {len(daily_complete)} days\")\n",
        "print(f\"  Date range: {daily_complete['Date'].min()} to {daily_complete['Date'].max()}\")\n",
        "print(f\"  Missing days filled: {len(daily_complete) - len(daily_data)}\")\n",
        "\n",
        "# 2. Monthly aggregation\n",
        "monthly_data = df.groupby(df['InvoiceDate'].dt.to_period('M')).agg({\n",
        "    'TotalPrice': 'sum',\n",
        "    'Quantity': 'sum',\n",
        "    'InvoiceNo': 'nunique',\n",
        "    'CustomerID': 'nunique'\n",
        "}).reset_index()\n",
        "\n",
        "monthly_data.columns = ['YearMonth', 'MonthlyRevenue', 'MonthlyQuantity', 'MonthlyTransactions', 'MonthlyCustomers']\n",
        "monthly_data['Date'] = pd.to_datetime(monthly_data['YearMonth'].astype(str))\n",
        "print(f\"\\n  Monthly time-series: {len(monthly_data)} months\")\n",
        "\n",
        "# 3. Customer-level features for behavior prediction\n",
        "print(\"\\n2. CUSTOMER-LEVEL FEATURES:\")\n",
        "reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "# Use named aggregations to avoid duplicate column keys and ensure clean feature names\n",
        "customer_features = df.groupby('CustomerID').agg(\n",
        "    Recency=('InvoiceDate', lambda x: (reference_date - x.max()).days),  # days since last purchase\n",
        "    Frequency=('InvoiceNo', 'nunique'),                                  # number of distinct invoices\n",
        "    TotalSpent=('TotalPrice', 'sum'),                                    # total monetary value\n",
        "    AvgTransaction=('TotalPrice', 'mean'),                               # average basket value\n",
        "    TotalQuantity=('Quantity', 'sum'),                                   # total units purchased\n",
        "    FirstPurchase=('InvoiceDate', 'min'),                                # first purchase date\n",
        "    LastPurchase=('InvoiceDate', 'max')                                  # most recent purchase date\n",
        ").reset_index()\n",
        "\n",
        "customer_features['CustomerLifetime'] = (customer_features['LastPurchase'] - customer_features['FirstPurchase']).dt.days\n",
        "customer_features['AvgDaysBetweenPurchases'] = customer_features['CustomerLifetime'] / customer_features['Frequency']\n",
        "\n",
        "print(f\"  Customer features: {len(customer_features)} customers\")\n",
        "print(f\"  Features: Recency, Frequency, Monetary, Lifetime, AvgDaysBetweenPurchases\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample Time-Series Data (Last 10 days):\")\n",
        "print(daily_complete[['Date', 'DailyRevenue', 'DailyTransactions']].tail(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nSample Customer Features (Top 10 by Total Spent):\")\n",
        "print(customer_features.nlargest(10, 'TotalSpent')[['CustomerID', 'Recency', 'Frequency', 'TotalSpent']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA PREPARATION COMPLETE\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "metrics = {\n",
        "    \"Time Series Forecasting\": {\n",
        "        \"MAE (Mean Absolute Error)\": \"Average absolute difference between predicted and actual values\",\n",
        "        \"RMSE (Root Mean Squared Error)\": \"Penalizes larger errors more, good for business impact\",\n",
        "        \"MAPE (Mean Absolute Percentage Error)\": \"Percentage error, interpretable for stakeholders\",\n",
        "        \"R² (Coefficient of Determination)\": \"Proportion of variance explained by the model\"\n",
        "    },\n",
        "    \"Classification (Churn/Purchase Prediction)\": {\n",
        "        \"Accuracy\": \"Overall correctness of predictions\",\n",
        "        \"Precision\": \"Proportion of positive predictions that are correct\",\n",
        "        \"Recall\": \"Proportion of actual positives correctly identified\",\n",
        "        \"F1-Score\": \"Harmonic mean of precision and recall\",\n",
        "        \"ROC-AUC\": \"Area under ROC curve, measures classification performance\"\n",
        "    },\n",
        "    \"Regression (Basket Size)\": {\n",
        "        \"MAE\": \"Average absolute error in basket size prediction\",\n",
        "        \"RMSE\": \"Penalizes larger errors\",\n",
        "        \"R²\": \"Model fit quality\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for category, metric_dict in metrics.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    print(\"-\" * 60)\n",
        "    for metric, description in metric_dict.items():\n",
        "        print(f\"  • {metric}: {description}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Modeling Approach & Implementation Plan\n",
        "\n",
        "Document the step-by-step approach for Phase 3 implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementation plan\n",
        "print(\"=\" * 80)\n",
        "print(\"MODELING APPROACH & IMPLEMENTATION PLAN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "plan = {\n",
        "    \"Phase 1: Baseline Models\": [\n",
        "        \"1. Implement ARIMA for daily revenue forecasting\",\n",
        "        \"2. Train baseline logistic regression for churn prediction\",\n",
        "        \"3. Evaluate baseline models using defined metrics\",\n",
        "        \"4. Document baseline performance\"\n",
        "    ],\n",
        "    \"Phase 2: Advanced Models\": [\n",
        "        \"1. Implement Prophet for revenue forecasting with seasonality\",\n",
        "        \"2. Train Random Forest for purchase likelihood prediction\",\n",
        "        \"3. Compare advanced models with baselines\",\n",
        "        \"4. Feature engineering and hyperparameter tuning\"\n",
        "    ],\n",
        "    \"Phase 3: Model Validation\": [\n",
        "        \"1. Time-series cross-validation (walk-forward validation)\",\n",
        "        \"2. Hold-out test set evaluation\",\n",
        "        \"3. Statistical significance testing\",\n",
        "        \"4. Business impact assessment\"\n",
        "    ],\n",
        "    \"Phase 4: Model Deployment\": [\n",
        "        \"1. Model serialization and versioning\",\n",
        "        \"2. Prediction pipeline development\",\n",
        "        \"3. Model monitoring framework\",\n",
        "        \"4. Documentation and reporting\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for phase, steps in plan.items():\n",
        "    print(f\"\\n{phase}:\")\n",
        "    print(\"-\" * 60)\n",
        "    for step in steps:\n",
        "        print(f\"  {step}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY CONSIDERATIONS:\")\n",
        "print(\"=\" * 80)\n",
        "considerations = [\n",
        "    \"Train/Test Split: Use temporal split (e.g., last 3 months as test set)\",\n",
        "    \"Cross-Validation: Time-series cross-validation to avoid data leakage\",\n",
        "    \"Feature Engineering: Create lag features, rolling statistics, temporal features\",\n",
        "    \"Model Interpretability: Balance accuracy with interpretability for business stakeholders\",\n",
        "    \"Scalability: Ensure models can handle production-scale data\",\n",
        "    \"Monitoring: Plan for model performance monitoring and retraining\"\n",
        "]\n",
        "\n",
        "for i, consideration in enumerate(considerations, 1):\n",
        "    print(f\"{i}. {consideration}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PREDICTIVE MODELING PLANNING COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nNext Steps (Phase 3):\")\n",
        "print(\"  1. Implement baseline ARIMA model\")\n",
        "print(\"  2. Implement Prophet model\")\n",
        "print(\"  3. Build customer churn prediction model\")\n",
        "print(\"  4. Evaluate and compare all models\")\n",
        "print(\"  5. Document results and business implications\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Data Visualization for Modeling Preparation\n",
        "\n",
        "Visualize prepared datasets to understand their characteristics for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize prepared datasets\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA VISUALIZATION FOR MODELING PREPARATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Time-series data visualization\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "fig.suptitle('Predictive Modeling - Data Preparation Visualization', fontsize=16, y=0.995)\n",
        "\n",
        "# Daily revenue time-series\n",
        "axes[0, 0].plot(daily_complete['Date'], daily_complete['DailyRevenue'], \n",
        "               linewidth=1.5, color='steelblue', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Date', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Daily Revenue (£)', fontsize=12)\n",
        "axes[0, 0].set_title('Daily Revenue Time-Series', fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Monthly revenue time-series\n",
        "axes[0, 1].plot(monthly_data['Date'], monthly_data['MonthlyRevenue'], \n",
        "               marker='o', linewidth=2, markersize=6, color='coral')\n",
        "axes[0, 1].set_xlabel('Date', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Monthly Revenue (£)', fontsize=12)\n",
        "axes[0, 1].set_title('Monthly Revenue Time-Series', fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Daily revenue distribution\n",
        "axes[1, 0].hist(daily_complete['DailyRevenue'], bins=50, edgecolor='black', alpha=0.7, color='teal')\n",
        "axes[1, 0].set_xlabel('Daily Revenue (£)', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 0].set_title('Daily Revenue Distribution', fontweight='bold')\n",
        "axes[1, 0].axvline(x=daily_complete['DailyRevenue'].mean(), color='red', linestyle='--', \n",
        "                  linewidth=2, label=f'Mean: £{daily_complete[\"DailyRevenue\"].mean():,.0f}')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Monthly revenue distribution\n",
        "axes[1, 1].hist(monthly_data['MonthlyRevenue'], bins=20, edgecolor='black', alpha=0.7, color='purple')\n",
        "axes[1, 1].set_xlabel('Monthly Revenue (£)', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 1].set_title('Monthly Revenue Distribution', fontweight='bold')\n",
        "axes[1, 1].axvline(x=monthly_data['MonthlyRevenue'].mean(), color='red', linestyle='--',\n",
        "                  linewidth=2, label=f'Mean: £{monthly_data[\"MonthlyRevenue\"].mean():,.0f}')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Customer features distribution\n",
        "axes[2, 0].scatter(customer_features['Recency'], customer_features['Frequency'],\n",
        "                  c=customer_features['TotalSpent'], cmap='viridis', alpha=0.6, s=20, edgecolors='black', linewidth=0.3)\n",
        "axes[2, 0].set_xlabel('Recency (days)', fontsize=12)\n",
        "axes[2, 0].set_ylabel('Frequency (transactions)', fontsize=12)\n",
        "axes[2, 0].set_title('Customer Features: Recency vs Frequency\\n(Color = Total Spent)', fontweight='bold')\n",
        "axes[2, 0].set_xscale('log')\n",
        "axes[2, 0].set_yscale('log')\n",
        "axes[2, 0].grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(axes[2, 0].collections[0], ax=axes[2, 0])\n",
        "cbar.set_label('Total Spent (£)', fontsize=10)\n",
        "\n",
        "# Customer monetary value distribution\n",
        "axes[2, 1].hist(customer_features['TotalSpent'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[2, 1].set_xlabel('Total Spent (£)', fontsize=12)\n",
        "axes[2, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[2, 1].set_title('Customer Monetary Value Distribution', fontweight='bold')\n",
        "axes[2, 1].set_xscale('log')\n",
        "axes[2, 1].axvline(x=customer_features['TotalSpent'].median(), color='red', linestyle='--',\n",
        "                  linewidth=2, label=f'Median: £{customer_features[\"TotalSpent\"].median():,.0f}')\n",
        "axes[2, 1].legend()\n",
        "axes[2, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Summary statistics visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "fig.suptitle('Data Summary Statistics', fontsize=14, y=1.02)\n",
        "\n",
        "# Time-series statistics comparison\n",
        "ts_stats = pd.DataFrame({\n",
        "    'Metric': ['Mean', 'Median', 'Std', 'Min', 'Max'],\n",
        "    'Daily Revenue': [\n",
        "        daily_complete['DailyRevenue'].mean(),\n",
        "        daily_complete['DailyRevenue'].median(),\n",
        "        daily_complete['DailyRevenue'].std(),\n",
        "        daily_complete['DailyRevenue'].min(),\n",
        "        daily_complete['DailyRevenue'].max()\n",
        "    ],\n",
        "    'Monthly Revenue': [\n",
        "        monthly_data['MonthlyRevenue'].mean(),\n",
        "        monthly_data['MonthlyRevenue'].median(),\n",
        "        monthly_data['MonthlyRevenue'].std(),\n",
        "        monthly_data['MonthlyRevenue'].min(),\n",
        "        monthly_data['MonthlyRevenue'].max()\n",
        "    ]\n",
        "})\n",
        "\n",
        "x_pos = np.arange(len(ts_stats))\n",
        "width = 0.35\n",
        "axes[0].bar(x_pos - width/2, ts_stats['Daily Revenue'], width, label='Daily', alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0].bar(x_pos + width/2, ts_stats['Monthly Revenue'], width, label='Monthly', alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[0].set_xlabel('Statistic', fontsize=12)\n",
        "axes[0].set_ylabel('Value (£)', fontsize=12)\n",
        "axes[0].set_title('Time-Series Statistics Comparison', fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(ts_stats['Metric'], rotation=45)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "axes[0].set_yscale('log')\n",
        "\n",
        "# Customer feature statistics\n",
        "customer_stats = pd.DataFrame({\n",
        "    'Feature': ['Recency', 'Frequency', 'Total Spent'],\n",
        "    'Mean': [\n",
        "        customer_features['Recency'].mean(),\n",
        "        customer_features['Frequency'].mean(),\n",
        "        customer_features['TotalSpent'].mean()\n",
        "    ],\n",
        "    'Median': [\n",
        "        customer_features['Recency'].median(),\n",
        "        customer_features['Frequency'].median(),\n",
        "        customer_features['TotalSpent'].median()\n",
        "    ]\n",
        "})\n",
        "\n",
        "x_pos = np.arange(len(customer_stats))\n",
        "axes[1].bar(x_pos - width/2, customer_stats['Mean'], width, label='Mean', alpha=0.7, color='teal', edgecolor='black')\n",
        "axes[1].bar(x_pos + width/2, customer_stats['Median'], width, label='Median', alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1].set_xlabel('Feature', fontsize=12)\n",
        "axes[1].set_ylabel('Value', fontsize=12)\n",
        "axes[1].set_title('Customer Feature Statistics', fontweight='bold')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(customer_stats['Feature'], rotation=45)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA VISUALIZATION COMPLETE\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: ARIMA Time Series Modeling (Daily Revenue)\n",
        "\n",
        "We now implement a **baseline ARIMA model** on the prepared **daily revenue** time series:\n",
        "- Use `daily_complete` and the `DailyRevenue` column as the univariate series\n",
        "- Check stationarity using the Augmented Dickey-Fuller (ADF) test\n",
        "- Inspect ACF and PACF plots to guide (p, d, q) selection\n",
        "- Split the data into train and test sets using a temporal split\n",
        "- Fit an ARIMA model on the training set and forecast the test period\n",
        "- Evaluate performance using MAE, RMSE, and MAPE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ARIMA: prepare series and stationarity checks ===\n",
        "\n",
        "# Build univariate daily revenue series\n",
        "ts = daily_complete.set_index('Date')['DailyRevenue'].asfreq('D')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ARIMA MODELING - DAILY REVENUE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Number of observations: {len(ts)}\")\n",
        "print(f\"Start date: {ts.index.min()} | End date: {ts.index.max()}\")\n",
        "\n",
        "# Plot original series\n",
        "fig, ax = plt.subplots(figsize=(14, 4))\n",
        "ax.plot(ts, color='steelblue')\n",
        "ax.set_title(\"Daily Revenue Time Series\", fontweight='bold')\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Revenue (£)\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# ADF helper\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def adf_test(series, title=\"\"):\n",
        "    print(f\"\\nADF Test: {title}\")\n",
        "    result = adfuller(series.dropna(), autolag='AIC')\n",
        "    labels = ['ADF Statistic', 'p-value', '# Lags Used', '# Observations Used']\n",
        "    out = dict(zip(labels, result[0:4]))\n",
        "    for k, v in out.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    for key, value in result[4].items():\n",
        "        print(f\"  Critical Value {key}: {value:.4f}\")\n",
        "    if out['p-value'] < 0.05:\n",
        "        print(\"  => Reject H0: Series is likely STATIONARY.\")\n",
        "    else:\n",
        "        print(\"  => Fail to reject H0: Series is likely NON-STATIONARY.\")\n",
        "\n",
        "# Stationarity tests\n",
        "adf_test(ts, title=\"DailyRevenue (Original)\")\n",
        "\n",
        "ts_diff = ts.diff().dropna()\n",
        "adf_test(ts_diff, title=\"DailyRevenue (1st Difference)\")\n",
        "\n",
        "# ACF and PACF on differenced series\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "plot_acf(ts_diff, ax=axes[0], lags=30)\n",
        "axes[0].set_title(\"ACF - Differenced Daily Revenue\")\n",
        "plot_pacf(ts_diff, ax=axes[1], lags=30, method='ywm')\n",
        "axes[1].set_title(\"PACF - Differenced Daily Revenue\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === ARIMA: train/test split, fit and evaluation ===\n",
        "\n",
        "# Temporal split (last 90 days as test)\n",
        "test_size = 90\n",
        "train_ts = ts.iloc[:-test_size]\n",
        "test_ts = ts.iloc[-test_size:]\n",
        "\n",
        "print(\"\\nTrain/Test Split (ARIMA):\")\n",
        "print(f\"  Train period: {train_ts.index.min()} to {train_ts.index.max()} ({len(train_ts)} days)\")\n",
        "print(f\"  Test period : {test_ts.index.min()} to {test_ts.index.max()} ({len(test_ts)} days)\")\n",
        "\n",
        "order = (1, 1, 1)  # baseline choice; can be tuned\n",
        "print(f\"\\nFitting ARIMA model with order={order} ...\")\n",
        "model = ARIMA(train_ts, order=order)\n",
        "model_fit = model.fit()\n",
        "print(model_fit.summary())\n",
        "\n",
        "# Forecast over test period\n",
        "start = len(train_ts)\n",
        "end = len(train_ts) + len(test_ts) - 1\n",
        "forecast = model_fit.predict(start=start, end=end, typ='levels')\n",
        "forecast.index = test_ts.index\n",
        "\n",
        "# Metrics\n",
        "mae = mean_absolute_error(test_ts, forecast)\n",
        "rmse = np.sqrt(mean_squared_error(test_ts, forecast))\n",
        "mape = np.mean(np.abs((test_ts - forecast) / test_ts.replace(0, np.nan))) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ARIMA MODEL PERFORMANCE (Daily Revenue)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"MAE : {mae:,.2f}\")\n",
        "print(f\"RMSE: {rmse:,.2f}\")\n",
        "print(f\"MAPE: {mape:,.2f}%\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Plot actual vs forecast\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(train_ts.index, train_ts, label='Train', color='gray', alpha=0.6)\n",
        "plt.plot(test_ts.index, test_ts, label='Actual (Test)', color='steelblue')\n",
        "plt.plot(forecast.index, forecast, label='Forecast (ARIMA)', color='coral', linestyle='--')\n",
        "plt.title(\"ARIMA Daily Revenue Forecast - Train/Test\", fontweight='bold')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Revenue (£)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Error distribution\n",
        "errors = test_ts - forecast\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
        "plt.title(\"Forecast Error Distribution (ARIMA Test Set)\", fontweight='bold')\n",
        "plt.xlabel(\"Error (£)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True, axis='y', alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Prophet Time Series Modeling (Daily Revenue)\n",
        "\n",
        "We now implement a **Prophet** model on the daily revenue series to capture trend and seasonality and compare its performance with ARIMA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Prophet: model fitting, forecasting and evaluation ===\n",
        "\n",
        "# Prepare data for Prophet: columns 'ds' (date) and 'y' (value)\n",
        "prophet_df = daily_complete[['Date', 'DailyRevenue']].rename(columns={'Date': 'ds', 'DailyRevenue': 'y'})\n",
        "prophet_df = prophet_df.sort_values('ds')\n",
        "\n",
        "# Use same temporal split as ARIMA\n",
        "prophet_train = prophet_df.iloc[:-test_size].copy()\n",
        "prophet_test = prophet_df.iloc[-test_size:].copy()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PROPHET MODELING - DAILY REVENUE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Train rows: {len(prophet_train)}, Test rows: {len(prophet_test)}\")\n",
        "\n",
        "m = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\n",
        "m.fit(prophet_train)\n",
        "\n",
        "future = prophet_test[['ds']].copy()\n",
        "forecast_prophet = m.predict(future)\n",
        "\n",
        "# Align predictions with test set\n",
        "y_true = prophet_test['y'].values\n",
        "y_pred = forecast_prophet['yhat'].values\n",
        "\n",
        "mae_p = mean_absolute_error(y_true, y_pred)\n",
        "rmse_p = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mape_p = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, np.nan, y_true))) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PROPHET MODEL PERFORMANCE (Daily Revenue)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"MAE : {mae_p:,.2f}\")\n",
        "print(f\"RMSE: {rmse_p:,.2f}\")\n",
        "print(f\"MAPE: {mape_p:,.2f}%\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Plot actual vs Prophet forecast\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(prophet_train['ds'], prophet_train['y'], label='Train', color='gray', alpha=0.6)\n",
        "plt.plot(prophet_test['ds'], prophet_test['y'], label='Actual (Test)', color='steelblue')\n",
        "plt.plot(forecast_prophet['ds'], forecast_prophet['yhat'], label='Forecast (Prophet)', color='green', linestyle='--')\n",
        "plt.title(\"Prophet Daily Revenue Forecast - Train/Test\", fontweight='bold')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Revenue (£)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Prophet components (trend/seasonality)\n",
        "fig = m.plot_components(forecast_prophet)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Customer Behavior Prediction (Churn / Activity)\n",
        "\n",
        "We now build a **baseline customer behavior model**:\n",
        "- Use the `customer_features` dataset (Recency, Frequency, Monetary, etc.)\n",
        "- Define a simple churn/active label based on **Recency**\n",
        "- Train a **logistic regression** classifier\n",
        "- Evaluate using accuracy, precision, recall, F1, and ROC-AUC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Customer behavior model: Random Forest & XGBoost ===\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier  # type: ignore\n",
        "    xgb_available = True\n",
        "except Exception as e:  # pragma: no cover\n",
        "    print(\"XGBoost not available in this environment. Skipping XGBoost model.\")\n",
        "    print(\"Error:\", e)\n",
        "    xgb_available = False\n",
        "\n",
        "# Reuse the same supervised dataset (X, y) from the logistic regression step\n",
        "X_rf = X\n",
        "y_rf = y\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(\n",
        "    X_rf, y_rf, test_size=0.3, random_state=42, stratify=y_rf\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CUSTOMER BEHAVIOR MODEL - RANDOM FOREST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    class_weight=\"balanced_subsample\"\n",
        ")\n",
        "rf.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "rf_pred = rf.predict(X_test_rf)\n",
        "rf_proba = rf.predict_proba(X_test_rf)[:, 1]\n",
        "\n",
        "print(\"Random Forest Classification Report (Churn=1)\")\n",
        "print(classification_report(y_test_rf := y_test_rf if 'y_test_rf' in globals() else y_test, rf_pred, digits=3))\n",
        "\n",
        "rf_roc_auc = roc_auc_score(y_test_rf, rf_proba)\n",
        "print(f\"Random Forest ROC-AUC: {rf_roc_auc:.3f}\")\n",
        "\n",
        "if xgb_available:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CUSTOMER BEHAVIOR MODEL - XGBOOST\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42\n",
        "    )\n",
        "    xgb.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "    xgb_pred = xgb.predict(X_test_rf)\n",
        "    xgb_proba = xgb.predict_proba(X_test_rf)[:, 1]\n",
        "\n",
        "    print(\"XGBoost Classification Report (Churn=1)\")\n",
        "    print(classification_report(y_test_rf, xgb_pred, digits=3))\n",
        "\n",
        "    xgb_roc_auc = roc_auc_score(y_test_rf, xgb_proba)\n",
        "    print(f\"XGBoost ROC-AUC: {xgb_roc_auc:.3f}\")\n",
        "\n",
        "print(\"\\nCompare the performance of Logistic Regression, Random Forest, and XGBoost to choose the best model for deployment.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Customer behavior model: churn vs active ===\n",
        "\n",
        "# Label definition: customers with recent purchase are \"active\" (0), others \"churned\" (1)\n",
        "# Here we mark churned if Recency > 90 days (tunable threshold)\n",
        "\n",
        "customer_model_df = customer_features.copy().dropna(subset=['Recency', 'Frequency', 'TotalSpent'])\n",
        "customer_model_df['Churned'] = (customer_model_df['Recency'] > 90).astype(int)\n",
        "\n",
        "X = customer_model_df[['Recency', 'Frequency', 'TotalSpent', 'AvgDaysBetweenPurchases']].fillna(0)\n",
        "y = customer_model_df['Churned']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CUSTOMER BEHAVIOR MODEL - LOGISTIC REGRESSION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "print(f\"Churn rate (overall): {y.mean():.3f}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nClassification Report (Churn=1)\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "im = ax.imshow(cm, cmap='Blues')\n",
        "ax.set_title('Confusion Matrix - Customer Churn Model', fontweight='bold')\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('True label')\n",
        "\n",
        "for (i, j), v in np.ndenumerate(cm):\n",
        "    ax.text(j, i, str(v), ha='center', va='center', color='black', fontweight='bold')\n",
        "\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(['Active (0)', 'Churned (1)'])\n",
        "ax.set_yticklabels(['Active (0)', 'Churned (1)'])\n",
        "plt.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Model Selection & Hyperparameter Tuning (Customer Behavior)\n",
        "\n",
        "To maximise predictive performance while avoiding overfitting, we compare and tune **Random Forest** and **XGBoost** on the churn label using **ROC-AUC** with stratified cross‑validation. The best‑performing model can then be promoted for deployment in the dashboard/backend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Model selection & hyperparameter tuning (ROC-AUC, CV) ===\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL SELECTION & HYPERPARAMETER TUNING - CUSTOMER BEHAVIOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use the full supervised dataset X, y defined above (Recency, Frequency, Monetary, etc.)\n",
        "X_tune = X.copy()\n",
        "y_tune = y.copy()\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Random Forest search space\n",
        "rf_base = RandomForestClassifier(random_state=42, class_weight=\"balanced_subsample\")\n",
        "rf_param_dist = {\n",
        "    \"n_estimators\": randint(150, 400),\n",
        "    \"max_depth\": randint(3, 12),\n",
        "    \"min_samples_split\": randint(2, 10),\n",
        "    \"min_samples_leaf\": randint(1, 6),\n",
        "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.8]\n",
        "}\n",
        "\n",
        "rf_search = RandomizedSearchCV(\n",
        "    rf_base,\n",
        "    param_distributions=rf_param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_search.fit(X_tune, y_tune)\n",
        "print(\"\\nBest Random Forest params:\", rf_search.best_params_)\n",
        "print(f\"Best CV ROC-AUC (RF): {rf_search.best_score_:.3f}\")\n",
        "\n",
        "best_rf = rf_search.best_estimator_\n",
        "\n",
        "if xgb_available:\n",
        "    # XGBoost search space\n",
        "    xgb_base = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        tree_method=\"hist\"\n",
        "    )\n",
        "\n",
        "    xgb_param_dist = {\n",
        "        \"n_estimators\": randint(200, 500),\n",
        "        \"max_depth\": randint(3, 8),\n",
        "        \"learning_rate\": uniform(0.01, 0.2),\n",
        "        \"subsample\": uniform(0.6, 0.4),\n",
        "        \"colsample_bytree\": uniform(0.6, 0.4)\n",
        "    }\n",
        "\n",
        "    xgb_search = RandomizedSearchCV(\n",
        "        xgb_base,\n",
        "        param_distributions=xgb_param_dist,\n",
        "        n_iter=20,\n",
        "        scoring=\"roc_auc\",\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    xgb_search.fit(X_tune, y_tune)\n",
        "    print(\"\\nBest XGBoost params:\", xgb_search.best_params_)\n",
        "    print(f\"Best CV ROC-AUC (XGB): {xgb_search.best_score_:.3f}\")\n",
        "\n",
        "    best_xgb = xgb_search.best_estimator_\n",
        "\n",
        "print(\"\\nSelect the model (RF or XGB) with the higher CV ROC-AUC as the final production model.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
