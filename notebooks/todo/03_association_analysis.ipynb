{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Association Rule Mining\n",
        "\n",
        "This notebook performs market basket analysis to find products frequently bought together.\n",
        "\n",
        "## Objectives\n",
        "- Prepare transaction data for association rule mining\n",
        "- Apply Apriori algorithm\n",
        "- Apply FP-Growth algorithm (optional)\n",
        "- Generate association rules\n",
        "- Calculate support, confidence, and lift metrics\n",
        "- Visualize product associations\n",
        "- Interpret rules for business insights\n",
        "\n",
        "## Phase 2 Requirements\n",
        "- ✅ Run Apriori/FPGrowth algorithms\n",
        "- ✅ Generate association rules\n",
        "- ✅ Basic interpretation of top rules\n",
        "- ⚠️ Full analysis in Phase 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_association = \u001b[43mdf\u001b[49m[[\u001b[33m'\u001b[39m\u001b[33mInvoiceNo\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m      2\u001b[39m df_association.dropna(subset=[\u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m df_association[\u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m] = df_association[\u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m].str.strip()\n",
            "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Load required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ASSOCIATION RULE MINING - MARKET BASKET ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load preprocessed data (assuming df is available from preprocessing notebook)\n",
        "# If running standalone, load the cleaned dataset\n",
        "import os\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "data_path = os.path.join(project_root, 'data', 'raw', 'Online Retail.csv')\n",
        "\n",
        "# Load and basic clean\n",
        "df = pd.read_csv(data_path, encoding='latin-1')\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]  # Remove canceled orders\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "df = df[df['Description'].notna()]\n",
        "\n",
        "print(f\"\\nDataset loaded: {df.shape[0]:,} transactions\")\n",
        "print(f\"Unique invoices: {df['InvoiceNo'].nunique():,}\")\n",
        "print(f\"Unique products: {df['Description'].nunique():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare Transaction Data\n",
        "\n",
        "Convert transaction data into format suitable for association rule mining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare transaction data\n",
        "df_association = df[['InvoiceNo', 'Description']].copy()\n",
        "df_association.dropna(subset=['Description'], inplace=True)\n",
        "df_association['Description'] = df_association['Description'].str.strip()\n",
        "df_association = df_association[df_association['Description'] != '']\n",
        "\n",
        "# Group items by invoice to create baskets\n",
        "basket = df_association.groupby('InvoiceNo')['Description'].apply(list).reset_index()\n",
        "basket.columns = ['InvoiceNo', 'Items']\n",
        "\n",
        "print(f\"Total baskets (invoices): {len(basket):,}\")\n",
        "print(f\"Average items per basket: {basket['Items'].apply(len).mean():.2f}\")\n",
        "print(f\"Median items per basket: {basket['Items'].apply(len).median():.2f}\")\n",
        "\n",
        "# Display sample baskets\n",
        "print(\"\\nSample Baskets:\")\n",
        "display(basket.head(10))\n",
        "\n",
        "# Convert to list of lists format for TransactionEncoder\n",
        "transactions = basket['Items'].tolist()\n",
        "print(f\"\\nTotal transactions prepared: {len(transactions):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Encode Transactions\n",
        "\n",
        "Use TransactionEncoder to convert transaction lists into binary matrix format required by Apriori/FPGrowth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode transactions into binary matrix\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "print(f\"Encoded matrix shape: {df_encoded.shape}\")\n",
        "print(f\"Total unique items: {len(te.columns_)}\")\n",
        "print(f\"\\nSample of encoded matrix (first 5 rows, first 10 columns):\")\n",
        "display(df_encoded.iloc[:5, :10])\n",
        "\n",
        "# Check sparsity\n",
        "sparsity = 1 - df_encoded.sum().sum() / (df_encoded.shape[0] * df_encoded.shape[1])\n",
        "print(f\"\\nMatrix sparsity: {sparsity:.2%} (typical for retail data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Frequent Itemset Mining - Apriori Algorithm\n",
        "\n",
        "Apply Apriori algorithm to find frequent itemsets with minimum support threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Apriori algorithm\n",
        "# Start with a conservative support threshold (0.01 = 1% of transactions)\n",
        "min_support = 0.01\n",
        "print(f\"Applying Apriori algorithm with min_support = {min_support} ({min_support*100}%)\")\n",
        "\n",
        "frequent_itemsets = apriori(df_encoded, min_support=min_support, use_colnames=True, verbose=1)\n",
        "\n",
        "print(f\"\\nFrequent itemsets found: {len(frequent_itemsets):,}\")\n",
        "print(f\"\\nTop 10 frequent itemsets by support:\")\n",
        "display(frequent_itemsets.nlargest(10, 'support'))\n",
        "\n",
        "# Analyze itemset sizes\n",
        "frequent_itemsets['itemset_length'] = frequent_itemsets['itemsets'].apply(len)\n",
        "print(f\"\\nItemset size distribution:\")\n",
        "print(frequent_itemsets['itemset_length'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate Association Rules\n",
        "\n",
        "Generate association rules from frequent itemsets and calculate metrics (confidence, lift, conviction).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate association rules\n",
        "# Filter for rules with minimum confidence\n",
        "min_confidence = 0.3\n",
        "print(f\"Generating association rules with min_confidence = {min_confidence} ({min_confidence*100}%)\")\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "print(f\"\\nAssociation rules generated: {len(rules):,}\")\n",
        "print(f\"\\nRules columns: {rules.columns.tolist()}\")\n",
        "\n",
        "# Display top rules by different metrics\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TOP 10 RULES BY LIFT (Strongest Associations)\")\n",
        "print(\"=\" * 80)\n",
        "top_lift = rules.nlargest(10, 'lift')[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'conviction']]\n",
        "for idx, row in top_lift.iterrows():\n",
        "    print(f\"\\nRule {idx+1}:\")\n",
        "    print(f\"  If customer buys: {', '.join(list(row['antecedents']))}\")\n",
        "    print(f\"  Then likely to buy: {', '.join(list(row['consequents']))}\")\n",
        "    print(f\"  Support: {row['support']:.3f} ({row['support']*100:.1f}%)\")\n",
        "    print(f\"  Confidence: {row['confidence']:.3f} ({row['confidence']*100:.1f}%)\")\n",
        "    print(f\"  Lift: {row['lift']:.3f} (rule is {row['lift']:.1f}x more likely than random)\")\n",
        "    print(f\"  Conviction: {row['conviction']:.3f}\")\n",
        "\n",
        "display(top_lift)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Rule Analysis and Interpretation\n",
        "\n",
        "Analyze and interpret association rules for business insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rule statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"ASSOCIATION RULES SUMMARY STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nTotal Rules: {len(rules):,}\")\n",
        "print(f\"\\nSupport Statistics:\")\n",
        "print(f\"  Mean: {rules['support'].mean():.4f}\")\n",
        "print(f\"  Median: {rules['support'].median():.4f}\")\n",
        "print(f\"  Min: {rules['support'].min():.4f}\")\n",
        "print(f\"  Max: {rules['support'].max():.4f}\")\n",
        "\n",
        "print(f\"\\nConfidence Statistics:\")\n",
        "print(f\"  Mean: {rules['confidence'].mean():.4f}\")\n",
        "print(f\"  Median: {rules['confidence'].median():.4f}\")\n",
        "print(f\"  Min: {rules['confidence'].min():.4f}\")\n",
        "print(f\"  Max: {rules['confidence'].max():.4f}\")\n",
        "\n",
        "print(f\"\\nLift Statistics:\")\n",
        "print(f\"  Mean: {rules['lift'].mean():.4f}\")\n",
        "print(f\"  Median: {rules['lift'].median():.4f}\")\n",
        "print(f\"  Min: {rules['lift'].min():.4f}\")\n",
        "print(f\"  Max: {rules['lift'].max():.4f}\")\n",
        "print(f\"  Rules with lift > 1.0 (positive association): {len(rules[rules['lift'] > 1.0]):,}\")\n",
        "print(f\"  Rules with lift > 2.0 (strong association): {len(rules[rules['lift'] > 2.0]):,}\")\n",
        "\n",
        "# Filter high-quality rules\n",
        "high_quality_rules = rules[(rules['lift'] > 1.5) & (rules['confidence'] > 0.5) & (rules['support'] > 0.02)]\n",
        "print(f\"\\nHigh-Quality Rules (lift>1.5, confidence>0.5, support>0.02): {len(high_quality_rules):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TOP 10 RULES BY CONFIDENCE\")\n",
        "print(\"=\" * 80)\n",
        "top_confidence = rules.nlargest(10, 'confidence')[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
        "display(top_confidence)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TOP 10 RULES BY SUPPORT\")\n",
        "print(\"=\" * 80)\n",
        "top_support = rules.nlargest(10, 'support')[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
        "display(top_support)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Association Rules Analysis', fontsize=16, y=0.995)\n",
        "\n",
        "# 1. Support vs Confidence scatter\n",
        "axes[0, 0].scatter(rules['support'], rules['confidence'], alpha=0.5, s=50, c=rules['lift'], \n",
        "                  cmap='viridis', edgecolors='black', linewidth=0.5)\n",
        "axes[0, 0].set_xlabel('Support')\n",
        "axes[0, 0].set_ylabel('Confidence')\n",
        "axes[0, 0].set_title('Support vs Confidence (colored by Lift)')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "plt.colorbar(axes[0, 0].collections[0], ax=axes[0, 0], label='Lift')\n",
        "\n",
        "# 2. Lift distribution\n",
        "axes[0, 1].hist(rules['lift'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[0, 1].axvline(x=1.0, color='red', linestyle='--', label='Lift = 1.0 (no association)')\n",
        "axes[0, 1].set_xlabel('Lift')\n",
        "axes[0, 1].set_ylabel('Number of Rules')\n",
        "axes[0, 1].set_title('Lift Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Confidence distribution\n",
        "axes[1, 0].hist(rules['confidence'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[1, 0].set_xlabel('Confidence')\n",
        "axes[1, 0].set_ylabel('Number of Rules')\n",
        "axes[1, 0].set_title('Confidence Distribution')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Support distribution\n",
        "axes[1, 1].hist(rules['support'], bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[1, 1].set_xlabel('Support')\n",
        "axes[1, 1].set_ylabel('Number of Rules')\n",
        "axes[1, 1].set_title('Support Distribution')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top rules visualization\n",
        "if len(high_quality_rules) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    top_20 = high_quality_rules.nlargest(20, 'lift')\n",
        "    \n",
        "    # Create rule labels\n",
        "    rule_labels = [f\"{', '.join(list(rule['antecedents']))[:30]} → {', '.join(list(rule['consequents']))[:30]}\" \n",
        "                   for _, rule in top_20.iterrows()]\n",
        "    \n",
        "    y_pos = np.arange(len(top_20))\n",
        "    ax.barh(y_pos, top_20['lift'], alpha=0.7, color='purple', edgecolor='black')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(rule_labels, fontsize=8)\n",
        "    ax.set_xlabel('Lift', fontsize=12)\n",
        "    ax.set_title('Top 20 Association Rules by Lift', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    ax.axvline(x=1.0, color='red', linestyle='--', label='Lift = 1.0')\n",
        "    ax.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business Interpretation\n",
        "print(\"=\" * 80)\n",
        "print(\"BUSINESS INSIGHTS FROM ASSOCIATION RULES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. PRODUCT AFFINITY INSIGHTS:\")\n",
        "print(\"   Association rules reveal which products are frequently purchased together.\")\n",
        "print(\"   This information is critical for:\")\n",
        "print(\"   - Product placement strategies (co-locate related items)\")\n",
        "print(\"   - Cross-selling opportunities (recommend associated products)\")\n",
        "print(\"   - Inventory management (stock related items together)\")\n",
        "\n",
        "print(\"\\n2. STOCK ALLOCATION IMPLICATIONS:\")\n",
        "if len(high_quality_rules) > 0:\n",
        "    print(f\"   - {len(high_quality_rules):,} strong associations identified\")\n",
        "    print(\"   - When one product in a rule sells, stock the associated product\")\n",
        "    print(\"   - High-lift rules indicate products that should be stocked together\")\n",
        "\n",
        "print(\"\\n3. MARKET BASKET OPTIMIZATION:\")\n",
        "print(\"   - Rules with high support indicate common purchase combinations\")\n",
        "print(\"   - Rules with high confidence indicate reliable cross-selling opportunities\")\n",
        "print(\"   - Rules with high lift indicate genuine product affinity (not just popularity)\")\n",
        "\n",
        "print(\"\\n4. KEY METRICS INTERPRETATION:\")\n",
        "print(\"   - Support: How often the rule occurs in transactions\")\n",
        "print(\"   - Confidence: Probability of buying consequent given antecedent\")\n",
        "print(\"   - Lift: How much more likely the rule is than random chance\")\n",
        "print(\"   - Lift > 1.0: Positive association (products complement each other)\")\n",
        "print(\"   - Lift < 1.0: Negative association (products rarely bought together)\")\n",
        "\n",
        "print(\"\\n5. LIMITATIONS FOR PHASE 2:\")\n",
        "print(\"   - Analysis based on historical patterns; may not predict future behavior\")\n",
        "print(\"   - Does not account for external factors (promotions, seasonality)\")\n",
        "print(\"   - Rules may be spurious if products are simply popular independently\")\n",
        "print(\"   - Full business validation requires domain expertise and testing\")\n",
        "\n",
        "# Export top rules for further analysis\n",
        "if len(high_quality_rules) > 0:\n",
        "    output_path = os.path.join(project_root, 'data', 'processed', 'top_association_rules.csv')\n",
        "    high_quality_rules.to_csv(output_path, index=False)\n",
        "    print(f\"\\n✓ Top association rules exported to: {output_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ASSOCIATION RULE MINING COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nNext Steps (Phase 3):\")\n",
        "print(\"  - Validate rules with business stakeholders\")\n",
        "print(\"  - Test rules in real-world scenarios\")\n",
        "print(\"  - Integrate with recommendation systems\")\n",
        "print(\"  - Monitor rule performance over time\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
